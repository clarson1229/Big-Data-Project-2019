{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml import Pipeline \n",
    "from pyspark.sql.functions import rand \n",
    "from pyspark.mllib.evaluation import MulticlassMetrics \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "path=\"Imdb/\"\n",
    "positiveFiles = [x for x in os.listdir(path + \"train/pos/\") if x.endswith(\".txt\")]\n",
    "negativeFiles = [x for x in os.listdir(path + \"train/neg/\") if x.endswith(\".txt\")]\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "posReviews, negReviews = [], []\n",
    "\n",
    "for posfile in positiveFiles:\n",
    "    with open(path + \"train/pos/\" + posfile, encoding= \"latin1\") as file:\n",
    "        posReviews.append(file.read())\n",
    "for negfile in negativeFiles:\n",
    "    with open(path + \"train/neg/\" + negfile, encoding= \"latin1\") as file:\n",
    "        negReviews.append(file.read())\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "reviews = pd.concat([\n",
    "    pd.DataFrame({\"review\":posReviews, \"label\":1, \"file\":positiveFiles}),\n",
    "    pd.DataFrame({\"review\":negReviews, \"label\":0, \"file\":negativeFiles})\n",
    "], ignore_index=True).sample(frac=1, random_state=5)\n",
    "\n",
    "reviews.head()\n",
    "\n",
    "mySchema = StructType([ StructField(\"file\", StringType(), True)\\\n",
    "                       ,StructField(\"label\", IntegerType(), True)\\\n",
    "                       ,StructField(\"review\", StringType(), True)])\n",
    "\n",
    "#convert the data to spark dataframe so that it can be split up and drop the file column \n",
    "reviews2 = spark.createDataFrame(reviews,schema=mySchema)\n",
    "reviews2 = reviews2.drop(\"file\")\n",
    "(trainSet, validationSet, testSet) = reviews2.randomSplit([0.90, 0.05, 0.05], seed = 2000)\n",
    "\n",
    "# duplicates training and testing dat for another tokenizer we used\n",
    "trainSet2= trainSet\n",
    "validationSet2 = validationSet\n",
    "testSet2 = testSet\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|label|              review|               words|                  tf|            features|label 2.o|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|    0|!!!!! POSSIBLE SP...|[!!!!!, possible,...|(65536,[732,991,1...|(65536,[732,991,1...|      0.0|\n",
      "|    0|\" It had to be Yo...|[\", it, had, to, ...|(65536,[14,338,10...|(65536,[14,338,10...|      0.0|\n",
      "|    0|\"54\" is a film ba...|[\"54\", is, a, fil...|(65536,[14,1020,1...|(65536,[14,1020,1...|      0.0|\n",
      "|    0|\"A Town Called He...|[\"a, town, called...|(65536,[356,731,1...|(65536,[356,731,1...|      0.0|\n",
      "|    0|\"A death at a col...|[\"a, death, at, a...|(65536,[543,1444,...|(65536,[543,1444,...|      0.0|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tells our tokenizer which columns to use and output to  \n",
    "tokenizer = Tokenizer(inputCol=\"review\", outputCol=\"words\")\n",
    "# Takes a set of terms and turns them into features vectors. The feature vectors being the words \n",
    "# in our review sentences. \n",
    "\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "# Takes out all words that do not apear more than 5 times in the data \n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) \n",
    "# Tells the pipline what column to output the classification label to. \n",
    "label_stringIdx = StringIndexer(inputCol = \"label\", outputCol = \"label 2.o\")\n",
    "# builds the pipeline with the parameters we just set up \n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])\n",
    "\n",
    "# Fits the model to data\n",
    "# sets parameters to the model \n",
    "pipelineFit = pipeline.fit(trainSet)\n",
    "# sets the data to the parameters \n",
    "trainDF = pipelineFit.transform(trainSet)\n",
    "validationDF = pipelineFit.transform(validationSet)\n",
    "trainDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92805132167358"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying logistic regression to our model\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "lrModel = lr.fit(trainDF)\n",
    "# Creates our predeictions and measures accuracy \n",
    "predictions = lrModel.transform(validationDF)\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9405383468739532\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "# same as our above model except the use of CountVectorizer \n",
    "# \n",
    "\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) \n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx])\n",
    "\n",
    "pipelineFit = pipeline.fit(trainSet2)\n",
    "trainDF = pipelineFit.transform(trainSet2)\n",
    "validationDF = pipelineFit.transform(validationSet2)\n",
    "lrModel = lr.fit(trainDF)\n",
    "\n",
    "predictions = lrModel.transform(validationDF)\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
