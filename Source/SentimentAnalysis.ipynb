{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml import Pipeline \n",
    "from pyspark.sql.functions import rand \n",
    "from pyspark.mllib.evaluation import MulticlassMetrics \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "path=\"Imdb/\"\n",
    "positiveFiles = [x for x in os.listdir(path + \"train/pos/\") if x.endswith(\".txt\")]\n",
    "negativeFiles = [x for x in os.listdir(path + \"train/neg/\") if x.endswith(\".txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "posReviews, negReviews = [], []\n",
    "\n",
    "for posfile in positiveFiles:\n",
    "    with open(path + \"train/pos/\" + posfile, encoding= \"latin1\") as file:\n",
    "        posReviews.append(file.read())\n",
    "for negfile in negativeFiles:\n",
    "    with open(path + \"train/neg/\" + negfile, encoding= \"latin1\") as file:\n",
    "        negReviews.append(file.read())\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews = pd.concat([\n",
    "    pd.DataFrame({\"review\":posReviews, \"label\":1, \"file\":positiveFiles}),\n",
    "    pd.DataFrame({\"review\":negReviews, \"label\":0, \"file\":negativeFiles})\n",
    "], ignore_index=True).sample(frac=1, random_state=5)\n",
    "\n",
    "reviews.head()\n",
    "\n",
    "mySchema = StructType([ StructField(\"file\", StringType(), True)\\\n",
    "                       ,StructField(\"label\", IntegerType(), True)\\\n",
    "                       ,StructField(\"review\", StringType(), True)])\n",
    "\n",
    "#convert the data to spark dataframe so that it can be split up and drop the file column \n",
    "reviews2 = spark.createDataFrame(reviews,schema=mySchema)\n",
    "reviews2 = reviews2.drop(\"file\")\n",
    "(trainSet, validationSet, testSet) = reviews2.randomSplit([0.90, 0.05, 0.05], seed = 2000)\n",
    "\n",
    "trainSet2= trainSet\n",
    "validationSet2 = validationSet\n",
    "testSet2 = testSet\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|label|              review|               words|                  tf|            features|label 2.o|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|    0|!!!!! POSSIBLE SP...|[!!!!!, possible,...|(65536,[732,991,1...|(65536,[732,991,1...|      0.0|\n",
      "|    0|\"A young woman un...|[\"a, young, woman...|(65536,[750,1217,...|(65536,[750,1217,...|      0.0|\n",
      "|    0|\"Black Angel\" is ...|[\"black, angel\", ...|(65536,[696,2888,...|(65536,[696,2888,...|      0.0|\n",
      "|    0|\"Fred Claus\" some...|[\"fred, claus\", s...|(65536,[14,61,315...|(65536,[14,61,315...|      0.0|\n",
      "|    0|\"Godzilla vs King...|[\"godzilla, vs, k...|(65536,[14,1114,1...|(65536,[14,1114,1...|      0.0|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"review\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) \n",
    "label_stringIdx = StringIndexer(inputCol = \"label\", outputCol = \"label 2.o\")\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])\n",
    "\n",
    "pipelineFit = pipeline.fit(trainSet)\n",
    "trainDF = pipelineFit.transform(trainSet)\n",
    "validationDF = pipelineFit.transform(validationSet)\n",
    "trainDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666146645865834"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(maxIter=100)\n",
    "lrModel = lr.fit(trainDF)\n",
    "predictions = lrModel.transform(validationDF)\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)\n",
    "\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(validationDF.count())\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8783151326053042\n",
      "ROC-AUC: 0.9342950906005575\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"review\", outputCol=\"words\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) \n",
    "label_stringIdx = StringIndexer(inputCol = \"label\", outputCol = \"label 2.0\")\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, lr])\n",
    "\n",
    "pipelineFit = pipeline.fit(trainSet2)\n",
    "predictions = pipelineFit.transform(validationSet2)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(validationSet2.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print (\"Accuracy Score: \" + str(accuracy))\n",
    "print (\"ROC-AUC: \" + str(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
